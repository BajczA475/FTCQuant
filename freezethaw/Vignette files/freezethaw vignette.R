#Freeze-thaw exploration--using a parameter sweep to understand the consequences of using different criteria for defining "effective freeze-thaw cycles" with respect to a response variable of interest. Here, that variable is soil Wet Aggregate Stabiity (WAS), and the data used are described in Boswell et al. 2020. We will also generate the heat maps we report in that publication. You may model your own analysis off of that provided here. 

#This will load all the necessary libraries. If you do not have them all, pause to download them first before continuing.

library("devtools")
install_github("BajczA475/freezethaw/freezethaw")
library("freezethaw")
library("ggplot2")
library("dplyr")
library("reshape2")
library("tidyr")
library("tibble")

#Load the data sheets--these are the modified versions I have cleaned up and they are included in the vignette folder.
data1 = read.csv("freeze1415.csv")
data2 = read.csv("freeze1516.csv")
data3 = read.csv("freeze1617.csv")
was.data = read.csv("was_data.csv")

#We first need to pack the data sheets into a list so they can be referenced more easily.
data.list = list(data1, data2, data3)

#We'll now perform the parameter sweep using freeze.thaw.analysis. The first argument is our packed list of data sets, and then the next three arguments are all given vectors that define the complete parameter space we would like to explore. Here, I provide the same parameter space we used in our publication referenced above (Boswell et al. 2020). This will take a few minutes to run on an average computer so you will need to be a little patient. It will report some progress as it runs to let you know it's still doing something. 
analysis.results = freeze.thaw.analysis(data.list = data.list, mag.vec = seq(from=0, to=0.5, by=0.1), dur.vec = 1:96, thres.vec = seq(from=0, to=-1, by=-0.1))

#Next, we'll extract the two outputs from the above function call into two separate objects.
Master.data.sheet = analysis.results$data
Definition.key = analysis.results$Definition.key

#Now, I needed to clean up the new data set by adding meta data from our particular data set so that the output later makes sense in context. I want to stress here that this code is totally specific to the data sheets provided here. THIS CODE IS NOT GENERIC. Any metadata you wish to add at this stage has to be done "manually."
Master.data.sheet = add_column(Master.data.sheet, Plot = rep(c(1,3,4,6,7,9,10,12,13,15,16,18), times=3), .before = "Def1") #Add a plot column
Master.data.sheet = add_column(Master.data.sheet, Veg = rep(rep(c("Prairie", "Agricultural"), each=6), times=3), .before = "Def1") #Add a vegetation type column
Master.data.sheet = add_column(Master.data.sheet, Treat = rep(c("N", "S"), times=18), .before = "Def1") #Add in the treatment column.
Master.data.sheet = add_column(Master.data.sheet, Year = rep(c(1,2,3), each=12), .after = "Treat") #Add in a year column.

#We'll now also append the WAS data from the other provided data sheet.
Master.data.sheet = add_column(Master.data.sheet, WAS = was.data$was, .before = "Def1")

##Next, we need to build up to a critical for loop that will assess which set of criteria leads to the highest fit between WAS and the FTC count data generated by our parameter sweep. Frankly, this part should be packed into its own function for convenience but that hasn't yet occurred.

#Make some storage vectors to store output.
betas = numeric(0)
Rsquareds = numeric(0)
pvals = numeric(0)

#This for loop may also take a few moments to run, but it should be much quicker than the previous one.
for(def in 6:ncol(Master.data.sheet)) { #NOTE THE 6 HERE--THIS ASSUMES THAT THE FIRST 5 COLUMNS IN THE MASTER.DATA.SHEET ARE NOW OCCUPIED BY COLUMNS OTHER THAN FTC DATA! IF THAT IS NOT TRUE, THIS FOR LOOP WILL NOT WORK UNLESS THIS 6 IS CHANGED TO THE APPROPRIATE COLUMN NUMBER!!
  
  tmp1 = lm(Master.data.sheet$WAS ~ Master.data.sheet[,def]) #NOTE WAS HERE! YOU WILL NEED TO SUBSTITUTE IN WHATEVER RESPONSE VARIABLE YOU ARE INTERESTED IN. This line of code runs a linear model, and whatever variable is referenced here will be the dependent variable in that model.
  
  beta = tmp1$coefficients[2] #Get the slope coefficient out of the model.
  R2 = summary(tmp1)$r.squared #Get the R squared value out of the model.
  if(!is.na(beta)) { pval = summary(tmp1)$coefficients[2,4] } else { pval = NA } # Get the p value of the model out.
  #Store all the above results in the output vectors.
  betas=c(betas, beta) 
  Rsquareds = c(Rsquareds, R2)
  pvals = c(pvals, pval)
}

#Next, we built a new data frame with all the results from the above for loop in an easier-to-read structure. Each row is a set of criteria values and each column is information about that set's performance.
summary.df = data.frame(Duration = Definition.key$Duration, Min_Magnitude = Definition.key$Min_Magnitude, 
                        Temp_Threshold = Definition.key$Temp_Threshold, Beta = betas, R2 = Rsquareds, Ps = pvals)

#This will identify which row (criteria set) has the highest R2 value. 
which(summary.df$R2 == max(summary.df$R2))


##One question we had was which parameter is the most/least "sensitive?" In other words, as we vary each parameter, does the R2 of the various models produced vary a lot or a little? These graphs plot the R squared values from every model (y axis) against the value of each parameter (x axis). It also shows the mean R2 value for all the models from each parameter value in purple. If you run these three graphs, you will see that the R2 varies a lot as the duration parameter is varied but almost not at all when the minimum magnitude parameter is varied.
temp.sum = group_by(summary.df, Temp_Threshold) %>% summarize(meanR2 = mean(R2))
duration.sum = group_by(summary.df, Duration) %>% summarize(meanR2 = mean(R2))
mag.sum = group_by(summary.df, Min_Magnitude) %>% summarize(meanR2 = mean(R2))

ggplot(summary.df, aes(y=R2, x=Temp_Threshold)) + geom_point(alpha=0.05, color="black", size=2) + theme_classic() + geom_point(data=temp.sum, aes(y=meanR2), shape = 23, size=3, color="purple", fill="purple") + geom_line(data=temp.sum, aes(y=meanR2), color="purple", size=1) + scale_x_continuous("Temperature threshold (degrees C)") + scale_y_continuous("R squared")

ggplot(summary.df, aes(y=R2, x=Duration)) + geom_point(alpha=0.05, color="black", size=2) + theme_classic() + geom_point(data=duration.sum, aes(y=meanR2), shape = 23, size=3, color="purple", fill="purple") + geom_line(data=duration.sum, aes(y=meanR2), color="purple", size=1) + scale_x_continuous("Duration (1/2 h)", breaks =c(0,25,50,75), labels=c(0,25,50,75)) + scale_y_continuous("R squared") 

ggplot(summary.df, aes(y=R2, x=Min_Magnitude)) + geom_point(alpha=0.05, color="black", size=2) + theme_classic()+ geom_point(data=mag.sum, aes(y=meanR2), shape = 23, size=3, color="purple", fill="purple") + geom_line(data=mag.sum, aes(y=meanR2), color="purple", size=1) + scale_x_continuous("Minimum magnitude offset (degrees C)") + scale_y_continuous("R squared")

##The other question we had was: How does the R2 value vary over the entire parameter space we explored? To address this question, we created a "heat map" using ggplot2 tools. The code here can be generalized except for the midpoint value I chose here, which I adjusted manually to be around the halfway point between the minimum and maximum R squared values we observed. You may have to adjust this value manually to suit your own needs.
ggplot(summary.df, aes(x=Temp_Threshold, y = Duration, fill=R2)) + geom_raster(interpolate = T) + facet_wrap(~Min_Magnitude) + scale_fill_gradient2(low="red", mid="black", high="green", midpoint = 0.25) 

#This will make a new data frame to plot a comparison scatterplot of the first definition (which is the most liberal parameter criteria, where any fluctation around 0 counts as a FTC) against the best-fitting one, as measured via R squared value. I entered the appropriate data here manually, but the code here could be adjusted to be more general.
comp.graph = Master.data.sheet[,c(1:6, 2602)]

#Here, you may need to adjust what your y and x data should be called to be tailored to your specific needs. As you can see from this graph, the "best-fitting" criteria set (blue) is a much tigher fit to the data than the "standard" criteria set is (red).
ggplot(comp.graph, aes(y=WAS)) + geom_point(aes(x=Def1), color="red", size=2) + geom_point(aes(x=Def2597), color="blue", size=2) + geom_smooth(method="lm", se=F, color="red", aes(x=Def1)) + geom_smooth(method="lm", se=F, color="blue", aes(x=Def2597))
